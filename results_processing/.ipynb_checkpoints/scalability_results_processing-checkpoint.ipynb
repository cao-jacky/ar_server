{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import operator\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "\n",
    "plt.rc('text', usetex=True)\n",
    "rc('font',**{'family':'serif','serif':['Times']})\n",
    "plt.rcParams['text.latex.preamble'] = [r'\\boldmath']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Searching for the log and error files and storing them into lists\n",
    "log_folder = \"../logs\"\n",
    "folder_files = []\n",
    "\n",
    "for r, d, f in os.walk(log_folder):\n",
    "    for file in f:\n",
    "        if '.txt' in file:\n",
    "            folder_files.append(os.path.join(r, file))\n",
    "\n",
    "log_files = []\n",
    "error_files = []\n",
    "            \n",
    "for curr_file in folder_files:\n",
    "    cf_orig = curr_file\n",
    "    curr_file = curr_file.split(\"/\")[2].split(\"_\")\n",
    "    \n",
    "    cf_type = curr_file[0]\n",
    "    cf_time = curr_file[1].split(\".txt\")[0]\n",
    "    \n",
    "    cft_obj = datetime.strptime(cf_time, '%Y-%m-%d %H:%M:%S')\n",
    "    cft_unix = cft_obj.timestamp()\n",
    "    \n",
    "    file_info = [cft_unix, cf_orig]\n",
    "    \n",
    "    globals()[str(cf_type)+\"_files\"].append(file_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def single_log_parser(log_file):    \n",
    "    # number of lines in file\n",
    "    with open(log_file) as f:\n",
    "        log_num_lines = sum(1 for _ in f)\n",
    "    \n",
    "    # no of frames dealt with\n",
    "    no_frames = open(log_file, 'r').read().count(\"Image size \")\n",
    "    \n",
    "    # read file contents of logs\n",
    "    lf_file = open(log_file, \"r\").readlines()\n",
    "    \n",
    "    # requests\n",
    "    requests = []\n",
    "    \n",
    "    # arrays for latencies which are in units of ms\n",
    "    session_latencies = np.zeros([no_frames, 15])\n",
    "    \n",
    "    cf_considered = 0\n",
    "    # looping over log\n",
    "    for lf_line in range(log_num_lines):\n",
    "        curr_line = lf_file[lf_line] \n",
    "        \n",
    "        if not curr_line == \"\\n\":\n",
    "            cl_split = curr_line.split(\" \")\n",
    "        \n",
    "        if \"Image size \" in curr_line:\n",
    "            if cf_considered not in requests:\n",
    "                requests.append(cf_considered)\n",
    "\n",
    "        if \"SIFT points extracted in time\" in curr_line:\n",
    "            try:\n",
    "                sift_time = float(cl_split[-1]) * 1000\n",
    "                session_latencies[cf_considered][3] = sift_time\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        if \"PCA encoding time\" in curr_line:\n",
    "            pca_time = float(cl_split[-1]) * 1000\n",
    "            session_latencies[cf_considered][4] = pca_time\n",
    "\n",
    "        if \"Fisher Vector encoding time\" in curr_line:\n",
    "            fsh_time = float(cl_split[-1]) * 1000\n",
    "            session_latencies[cf_considered][5] = fsh_time\n",
    "            \n",
    "        if \"LSH NN searching time\" in curr_line:\n",
    "            lshnn_time = float(cl_split[-1]) * 1000\n",
    "            session_latencies[cf_considered][6] = lshnn_time\n",
    "            \n",
    "        if \"MatchSiftData time\" in curr_line:\n",
    "            msd_time = float(cl_split[-2])\n",
    "            session_latencies[cf_considered][7] = msd_time\n",
    "            \n",
    "        if \"Matching features\" in curr_line:     \n",
    "            cf_considered += 1\n",
    "                \n",
    "    #print(session_latencies)\n",
    "    return session_latencies\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Client pre-processing: nan +- nan\n",
      "Data transfer: nan +- nan\n",
      "SIFT feature extraction: 1.250025 +- 3.0022105907219028\n",
      "PCA dimension reduction: 0.868797 +- 1.2357421878872346\n",
      "FV encoding with GMM: 8.936879999999999 +- 3.231433869737246\n",
      "LSH NN searching: 0.602007 +- 1.9920623388194951\n",
      "Template matching: 0.22 +- 0.4534841362532346\n",
      "Client post-processing: nan +- nan\n",
      "Overall latency: nan +- nan\n"
     ]
    }
   ],
   "source": [
    "# Sort by UNIX timestamp in ascending order\n",
    "log_files = sorted(log_files, key=operator.itemgetter(0))\n",
    "\n",
    "files_to_consider = log_files[-18:-1]\n",
    "\n",
    "results_array = np.zeros([1,15])\n",
    "\n",
    "for i in range(len(files_to_consider)):\n",
    "    curr_log = files_to_consider[i]\n",
    "    file_name = curr_log[1]\n",
    "\n",
    "    # Send file names to parser\n",
    "    log_results = single_log_parser(file_name)\n",
    "    lat_results = log_results\n",
    "\n",
    "    results_array = np.append(results_array, lat_results, axis=0)\n",
    "    \n",
    "results_array = results_array[~np.all(results_array == 0, axis=1)]\n",
    "#results_array[results_array == 0] = np.nan\n",
    "\n",
    "\n",
    "tasks = {\n",
    "    1 : \"Client pre-processing: \",\n",
    "    2 : \"Data transfer: \",\n",
    "    3 : \"SIFT feature extraction: \",\n",
    "    4 : \"PCA dimension reduction: \",\n",
    "    5 : \"FV encoding with GMM: \",\n",
    "    6 : \"LSH NN searching: \",\n",
    "    7 : \"Template matching: \",\n",
    "    8 : \"Client post-processing: \"\n",
    "}\n",
    "\n",
    "print(\"\")\n",
    "    \n",
    "overall_latency_med = 0\n",
    "overall_latency_std = 0\n",
    "for i in range(len(results_array[:,0])):\n",
    "    curr_index = i + 1 \n",
    "    if curr_index <= len(tasks):\n",
    "        curr_col = results_array[:,curr_index]\n",
    "        \n",
    "        # removing outliers\n",
    "        curr_col = curr_col[abs(curr_col - np.mean(curr_col)) < 2 * np.std(curr_col)]\n",
    "        print(np.shape(curr_col))\n",
    "        \n",
    "        med_val = np.nanmedian(curr_col)\n",
    "        std_val = np.nanstd(curr_col)\n",
    "        \n",
    "        if med_val == np.nan:\n",
    "            med_val = 0\n",
    "\n",
    "        overall_latency_med += med_val\n",
    "        overall_latency_std += std_val\n",
    "\n",
    "        print(tasks[curr_index] + str(med_val) + \" +- \" + str(std_val))\n",
    "\n",
    "print(\"Overall latency: \" + str(overall_latency_med) + \" +- \" + str(std_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
