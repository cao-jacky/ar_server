{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import operator\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "\n",
    "plt.rc('text', usetex=True)\n",
    "rc('font',**{'family':'serif','serif':['Times']})\n",
    "plt.rcParams['text.latex.preamble'] = [r'\\boldmath']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Searching for the log and error files and storing them into lists\n",
    "log_folder = \"../logs\"\n",
    "folder_files = []\n",
    "\n",
    "for r, d, f in os.walk(log_folder):\n",
    "    for file in f:\n",
    "        if '.txt' in file:\n",
    "            folder_files.append(os.path.join(r, file))\n",
    "\n",
    "log_files = []\n",
    "error_files = []\n",
    "            \n",
    "for curr_file in folder_files:\n",
    "    cf_orig = curr_file\n",
    "    curr_file = curr_file.split(\"/\")[2].split(\"_\")\n",
    "    \n",
    "    cf_type = curr_file[0]\n",
    "    cf_time = curr_file[1].split(\".txt\")[0]\n",
    "    \n",
    "    cft_obj = datetime.strptime(cf_time, '%Y-%m-%d %H:%M:%S')\n",
    "    cft_unix = cft_obj.timestamp()\n",
    "    \n",
    "    file_info = [cft_unix, cf_orig]\n",
    "    \n",
    "    globals()[str(cf_type)+\"_files\"].append(file_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Looking through the cache log and recognition log files \n",
    "\n",
    "def log_parser(cl_name, rl_name, lc_file):    \n",
    "    # number of lines in file\n",
    "    with open(cl_name) as f:\n",
    "        csf_num_lines = sum(1 for _ in f)\n",
    "    with open(rl_name) as g:\n",
    "        rsf_num_lines = sum(1 for _ in g)\n",
    "    with open(lc_file) as g:\n",
    "        lcf_num_lines = sum(1 for _ in g)\n",
    "    \n",
    "    # no of frames dealt with\n",
    "    no_frames = open(cl_name, 'r').read().count(\"res sent\")\n",
    "    \n",
    "    # read file contents of logs\n",
    "    cs_file = open(cl_name, \"r\").readlines()\n",
    "    rs_file = open(rl_name, \"r\").readlines()\n",
    "    logcat_file = open(lc_file, \"r\").readlines()\n",
    "    \n",
    "    # storing current frame being considered\n",
    "    cf_ticker = []\n",
    "    \n",
    "    # array for latencies which are in units of ms\n",
    "    session_latencies = np.zeros([no_frames, 9])\n",
    "    transfer_latencies = np.zeros([no_frames, 5])\n",
    "    \n",
    "    negative_results = []\n",
    "    \n",
    "    no_frames_considered = 0\n",
    "    # looping over cache log\n",
    "    for cs_line in range(csf_num_lines):\n",
    "        if not cs_line < 8:\n",
    "            if len(cf_ticker) == 4:\n",
    "                cf_ticker = []\n",
    "                no_frames_considered += 1\n",
    "            curr_line = cs_file[cs_line]\n",
    "            if curr_line == \"\\n\":\n",
    "                pass\n",
    "            else:\n",
    "                cl_split = curr_line.split(\" \")\n",
    "\n",
    "                curr_frame = cl_split[1]\n",
    "                cf_ticker.append(curr_frame)\n",
    "                \n",
    "                if len(cf_ticker) == 1:\n",
    "                    session_latencies[no_frames_considered][0] = curr_frame\n",
    "                    transfer_latencies[no_frames_considered][0] = curr_frame\n",
    "                    \n",
    "                transfer_latencies[no_frames_considered][len(cf_ticker)] = cl_split[-1]\n",
    "                \n",
    "                if \"res sent, marker#: 0\" in curr_line:\n",
    "                    negative_results.append(curr_frame)\n",
    "                #print(cl_split)\n",
    "    \n",
    "    # storing the index of where the current frame is, in session_latencies\n",
    "    sl_where = 0\n",
    "        \n",
    "    # looping over recognition log\n",
    "    for rs_line in range(rsf_num_lines):\n",
    "        if not rs_line < 5:\n",
    "            curr_line = rs_file[rs_line]\n",
    "            \n",
    "            if not curr_line == \"\\n\":\n",
    "                cl_split = curr_line.split(\" \")\n",
    "            \n",
    "            if \"received, filesize:\" in curr_line:\n",
    "                curr_frame_no = cl_split[1]\n",
    "                sl_where = np.where(session_latencies[:,0] == float(curr_frame_no))[0][0]\n",
    "                sift_extract_time = 0\n",
    "                \n",
    "            if not curr_frame_no in negative_results:\n",
    "                if \"ExtractSift time total = \" in curr_line:\n",
    "                    indiv_sift_time = float(re.search(\"ExtractSift time total =(.*)ms\", curr_line).group(1))\n",
    "                    sift_extract_time += indiv_sift_time\n",
    "\n",
    "                if \"sift res\" in curr_line:\n",
    "                    sift_time = float(cl_split[-1]) * 1000\n",
    "                    session_latencies[sl_where][3] = sift_time\n",
    "\n",
    "                    #print(sift_extract_time, sift_time)\n",
    "\n",
    "                if \"pca encoding time\" in curr_line:\n",
    "                    pca_time = float(cl_split[-1]) * 1000\n",
    "                    session_latencies[sl_where][4] = pca_time\n",
    "\n",
    "                if \"fisher encoding time\" in curr_line:\n",
    "                    fsh_time = float(cl_split[-1]) * 1000\n",
    "                    session_latencies[sl_where][5] = fsh_time\n",
    "\n",
    "                if \"MatchSiftData time\" in curr_line:\n",
    "                    msd_time = float(cl_split[-2]) \n",
    "                    session_latencies[sl_where][6] = msd_time\n",
    "\n",
    "                if \"FindHomography time\" in curr_line:\n",
    "                    fhy_time = float(cl_split[-2])\n",
    "                    session_latencies[sl_where][7] = fhy_time\n",
    "\n",
    "    # Client times\n",
    "    pp_begin_uxtime = 0\n",
    "    frame_sent_ux_time = 0 \n",
    "    curr_frame_no = 0\n",
    "    curr_frame_loc = 0\n",
    "    \n",
    "    # offset of phone in seconds\n",
    "    android_offset = 3.330\n",
    "    \n",
    "    for lg_line in range(lcf_num_lines):\n",
    "        curr_line = logcat_file[lg_line]\n",
    "        cl_split = curr_line.split(\" \")\n",
    "        \n",
    "        if \"get gray scaled frame data at\" in curr_line:\n",
    "            pp_begin_uxtime = float(cl_split[-1])\n",
    "        \n",
    "        if \"sent with size\" in curr_line:\n",
    "            # find frame number\n",
    "            frame_no = float(re.search(\"frame(.*)sent with size\", curr_line).group(1))\n",
    "            \n",
    "            if not frame_no in negative_results:\n",
    "                # check for existence of frame number in server results\n",
    "                sl_where = np.where(session_latencies[:,0] == frame_no)[0]\n",
    "\n",
    "                if not sl_where.size == 0:\n",
    "                    curr_frame_no = frame_no\n",
    "                    curr_frame_loc = sl_where[0]\n",
    "\n",
    "                    frame_sent_ux_time = float(cl_split[-1]) \n",
    "\n",
    "                    pp_time = frame_sent_ux_time - pp_begin_uxtime\n",
    "                    session_latencies[curr_frame_loc][1] = pp_time\n",
    "\n",
    "                    server_rec = transfer_latencies[curr_frame_loc][1]\n",
    "                    data_transfer = abs(server_rec - (frame_sent_ux_time/1000) - android_offset)\n",
    "                    #print((frame_sent_ux_time/1000),android_offset, server_rec)\n",
    "                    session_latencies[curr_frame_loc][2] = data_transfer\n",
    "\n",
    "        if \"res received at\" in curr_line:\n",
    "            client_rec = (float(cl_split[-1]) / 1000)\n",
    "\n",
    "        if \"OMX-VDEC-1080P: Video slvp perflock acquired\" in curr_line:\n",
    "            curr_date = cl_split[0]\n",
    "            curr_year = \"19\"\n",
    "\n",
    "            curr_time = cl_split[1]\n",
    "            dt_string = curr_year + \"-\" + curr_date + \" \" + curr_time\n",
    "            curr_dt = datetime.strptime(dt_string, '%y-%m-%d %H:%M:%S.%f')\n",
    "            cdt_unix = curr_dt.timestamp()\n",
    "\n",
    "            client_pp = cdt_unix - client_rec\n",
    "            session_latencies[curr_frame_loc][8] = client_pp\n",
    "                 \n",
    "    # Delete non-positive results rows\n",
    "    for nr_i in range(len(negative_results)):\n",
    "        curr_nr = negative_results[nr_i]\n",
    "        cnr_loc = np.where(session_latencies[:,0] == float(curr_nr))[0][0]\n",
    "        session_latencies = np.delete(session_latencies, cnr_loc, 0)\n",
    "                \n",
    "    return session_latencies\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current two log files are:\n",
      "[1570093332.0, '1', '../logs/log_1_2019-10-03 12:02:12.txt'] [1570093335.0, '0', '../logs/log_0_2019-10-03 12:02:15.txt']\n",
      "\n",
      "\n",
      "Client pre-processing: 12.5 +- 4.199999999999999\n",
      "Data transfer: 6.65553505897522 +- 0.039528036751684835\n",
      "SIFT feature extraction: 3.53193283081055 +- 1.3141686848760223\n",
      "PCA dimension reduction: 3.59857082366943 +- 0.9434871648887053\n",
      "FV encoding with GMM: 3.40092182159424 +- 0.8647845739730776\n",
      "LSH NN searching: 1.275 +- 0.16930445948054648\n",
      "Template matching: 4.665 +- 0.6511405378257447\n",
      "Client post-processing: 1.0830000638961792 +- 0.01390103476069095\n",
      "Overall latency: 36.709960598945614 +- 0.01390103476069095\n"
     ]
    }
   ],
   "source": [
    "# Parsing log files \n",
    "\n",
    "server_mode = {\n",
    "    '1': \"recognition server\",\n",
    "    '0': \"cache server\"\n",
    "}\n",
    "\n",
    "# Sort by UNIX timestamp in ascending order\n",
    "log_files = sorted(log_files, key=operator.itemgetter(0))\n",
    "\n",
    "# for cf_i in range(len(log_files)):\n",
    "#     if cf_i % 2 == 0:\n",
    "#         # only consider pairs of files together\n",
    "\n",
    "recog_server = log_files[-2]\n",
    "cache_server = log_files[-1]\n",
    "\n",
    "print(\"Current two log files are:\")\n",
    "print(recog_server, cache_server)\n",
    "\n",
    "rs_file = recog_server[2]\n",
    "cs_file = cache_server[2]\n",
    "\n",
    "# perform two checks to make sure correct files are being used\n",
    "rs_mode = recog_server[1]\n",
    "cs_mode = cache_server[1]\n",
    "\n",
    "if not (rs_mode == \"1\") and not (cs_mode == \"0\"):\n",
    "    print(\"An error occurred in servers start up, remove either following files to correct:\")\n",
    "    print(rs_file)\n",
    "    print(cs_file)\n",
    "    pass\n",
    "\n",
    "rs_time = recog_server[0]\n",
    "cs_time = cache_server[0]\n",
    "\n",
    "if not (cs_time - rs_time) < 10:\n",
    "    print(\"Servers were started too late apart - start them within 10 s of each other\")\n",
    "    pass\n",
    "\n",
    "# Collecting logcat file from Android Studio output\n",
    "logcat_file = \"/home/jacky/Desktop/mobile_ar_system/ar_apps/cloudridar_movieapp_client/logcat.txt\"\n",
    "\n",
    "# Send file names to parser\n",
    "parsed_results = log_parser(cs_file, rs_file, logcat_file)\n",
    "\n",
    "tasks = {\n",
    "    1 : \"Client pre-processing: \",\n",
    "    2 : \"Data transfer: \",\n",
    "    3 : \"SIFT feature extraction: \",\n",
    "    4 : \"PCA dimension reduction: \",\n",
    "    5 : \"FV encoding with GMM: \",\n",
    "    6 : \"LSH NN searching: \",\n",
    "    7 : \"Template matching: \",\n",
    "    8 : \"Client post-processing: \"\n",
    "}\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "overall_latency_med = 0\n",
    "overall_latency_std = 0\n",
    "for i in range(len(parsed_results[:,0])):\n",
    "    curr_index = i + 1 \n",
    "    if curr_index <= len(tasks):\n",
    "        med_val = np.median(parsed_results[:,curr_index])\n",
    "        std_val = np.std(parsed_results[:,curr_index])\n",
    "\n",
    "        overall_latency_med += med_val\n",
    "        overall_latency_std += std_val\n",
    "\n",
    "        print(tasks[curr_index] + str(med_val) + \" +- \" + str(std_val))\n",
    "\n",
    "print(\"Overall latency: \" + str(overall_latency_med) + \" +- \" + str(std_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "code_folding": [
     0,
     29,
     132
    ]
   },
   "outputs": [],
   "source": [
    "def single_log_parser(log_file, lc_file):    \n",
    "    # number of lines in file\n",
    "    with open(log_file) as f:\n",
    "        log_num_lines = sum(1 for _ in f)\n",
    "    with open(lc_file) as g:\n",
    "        lcf_num_lines = sum(1 for _ in g)\n",
    "    \n",
    "    # no of frames dealt with\n",
    "    no_frames = open(log_file, 'r').read().count(\"received, filesize\")\n",
    "    \n",
    "    # read file contents of logs\n",
    "    lf_file = open(log_file, \"r\").readlines()\n",
    "    logcat_file = open(lc_file, \"r\").readlines()\n",
    "    \n",
    "    # storing current frame being considered\n",
    "    cf_ticker = []\n",
    "    \n",
    "    # requests\n",
    "    requests = []\n",
    "    \n",
    "    # arrays for latencies which are in units of ms\n",
    "    session_latencies = np.zeros([no_frames, 15])\n",
    "    transfer_latencies = np.zeros([no_frames, 5])\n",
    "    \n",
    "    clients = []\n",
    "    negative_results=[]\n",
    "    \n",
    "    cf_considered = 0\n",
    "    # looping over log\n",
    "    for lf_line in range(log_num_lines):\n",
    "        curr_line = lf_file[lf_line] \n",
    "        \n",
    "        if not curr_line == \"\\n\":\n",
    "            cl_split = curr_line.split(\" \")\n",
    "        \n",
    "        if \"received, filesize:\" in curr_line:\n",
    "            curr_frame_no = cl_split[1]\n",
    "            device_ip = cl_split[-1]\n",
    "            dip_int = device_ip.replace('.', '').replace('\\n', '')\n",
    "            if dip_int not in clients:\n",
    "                clients.append(dip_int)\n",
    "            if session_latencies[cf_considered][0] == 0:\n",
    "                session_latencies[cf_considered][0] = float(curr_frame_no)\n",
    "                session_latencies[cf_considered][14] = float(dip_int)\n",
    "            # averaging multiple times together \n",
    "            sift_points = []\n",
    "            match_sift = []\n",
    "\n",
    "        if \"SIFT points extracted in time\" in curr_line:\n",
    "            sift_time = float(cl_split[-1]) * 1000\n",
    "            sift_points.append(sift_time)\n",
    "            sp_average = np.average(sift_points)\n",
    "            session_latencies[cf_considered][3] = sp_average\n",
    "\n",
    "        if \"PCA encoding time\" in curr_line:\n",
    "            pca_time = float(cl_split[-1]) * 1000\n",
    "            session_latencies[cf_considered][4] = pca_time\n",
    "\n",
    "        if \"Fisher Vector encoding time\" in curr_line:\n",
    "            fsh_time = float(cl_split[-1]) * 1000\n",
    "            session_latencies[cf_considered][5] = fsh_time\n",
    "            \n",
    "        if \"time before matching\" in curr_line:\n",
    "            tbf_timestamp = float(cl_split[-1])\n",
    "\n",
    "        if \"LSH NN searching time\" in curr_line:\n",
    "            lshnn_time = float(cl_split[-1]) * 1000\n",
    "            session_latencies[cf_considered][6] = lshnn_time\n",
    "            \n",
    "        if \"after matching\" in curr_line:\n",
    "            af_time = float(cl_split[-1])\n",
    "            fhy_time = (af_time - tbf_timestamp) * 1000\n",
    "            #session_latencies[cf_considered][7] = fhy_time\n",
    "            \n",
    "        if \"MatchSiftData time\" in curr_line:\n",
    "            msd_time = float(cl_split[-2])\n",
    "            match_sift.append(msd_time)\n",
    "            msd_average = np.average(msd_time)\n",
    "            session_latencies[cf_considered][7] = msd_average\n",
    "            \n",
    "        if \"Matching features\" in curr_line:\n",
    "            mf_percentage = float(cl_split[-2].replace('%', ''))\n",
    "            session_latencies[cf_considered][9] = mf_percentage\n",
    "            \n",
    "        # was the cache used and was it succesful\n",
    "        if \"Cache query - time before matching:\" in curr_line:\n",
    "            session_latencies[cf_considered][11] = 1\n",
    "        \n",
    "        if \"Added item to cache\" in curr_line: \n",
    "            session_latencies[cf_considered][13] = 1\n",
    "        \n",
    "        if \"res sent, marker#:\" in curr_line:\n",
    "            marker_no = float(cl_split[5])\n",
    "            session_latencies[cf_considered][10] = marker_no\n",
    "            \n",
    "            cache_query = session_latencies[cf_considered][11]\n",
    "            \n",
    "            mf_percentage = session_latencies[cf_considered][9]\n",
    "            if (cache_query) and (0 < mf_percentage < 100 ) and (marker_no == 1):\n",
    "                session_latencies[cf_considered][12] = 1\n",
    "                \n",
    "            cf_considered += 1\n",
    "\n",
    "    succesful_recognitions = []\n",
    "    succesful_cache = []\n",
    "    # search through all the frames\n",
    "    for i in range(len(session_latencies)):\n",
    "        curr_frame = session_latencies[i]\n",
    "        \n",
    "        cf_id = curr_frame[0]\n",
    "        cf_recog = curr_frame[10]\n",
    "        cf_cache = curr_frame[12] # cache queried\n",
    "        \n",
    "        cf_add_cache = curr_frame[13] # item was added to cache\n",
    "        \n",
    "        if int(cf_recog) == 1:\n",
    "            # frame has had succesful recognition performed or cache recognition\n",
    "            \n",
    "            if int(cf_add_cache):\n",
    "                succesful_recognitions.append(cf_id)\n",
    "            else:\n",
    "                succesful_cache.append(cf_id)\n",
    "        \n",
    "    # Client times\n",
    "    pp_begin_uxtime = 0\n",
    "    frame_sent_ux_time = 0 \n",
    "    curr_frame_no = 0\n",
    "    curr_frame_loc = 0\n",
    "    \n",
    "    # offset of phone in seconds\n",
    "    android_offset = 3.330\n",
    "    \n",
    "    for lg_line in range(lcf_num_lines):\n",
    "        curr_line = logcat_file[lg_line]\n",
    "        cl_split = curr_line.split(\" \")\n",
    "        \n",
    "        if \"get gray scaled frame data at\" in curr_line:\n",
    "            pp_begin_uxtime = float(cl_split[-1])\n",
    "        \n",
    "        if \"sent with size\" in curr_line:\n",
    "            # find frame number\n",
    "            frame_no = float(re.search(\"frame(.*)sent with size\", curr_line).group(1))\n",
    "            \n",
    "            if not frame_no in negative_results:\n",
    "                # check for existence of frame number in server results\n",
    "                sl_where = np.where(session_latencies[:,0] == frame_no)[0]\n",
    "\n",
    "                if not sl_where.size == 0:\n",
    "                    curr_frame_no = frame_no\n",
    "                    curr_frame_loc = sl_where[0]\n",
    "\n",
    "                    frame_sent_ux_time = float(cl_split[-1]) \n",
    "\n",
    "                    pp_time = frame_sent_ux_time - pp_begin_uxtime\n",
    "                    session_latencies[curr_frame_loc][1] = pp_time\n",
    "\n",
    "                    server_rec = transfer_latencies[curr_frame_loc][1]\n",
    "                    data_transfer = abs(server_rec - (frame_sent_ux_time/1000) - android_offset)\n",
    "                    #print((frame_sent_ux_time/1000),android_offset, server_rec)\n",
    "                    session_latencies[curr_frame_loc][2] = data_transfer\n",
    "\n",
    "        if \"res received at\" in curr_line:\n",
    "            client_rec = (float(cl_split[-1]) / 1000)\n",
    "\n",
    "        if \"OMX-VDEC-1080P: Video slvp perflock acquired\" in curr_line:\n",
    "            curr_date = cl_split[0]\n",
    "            curr_year = \"19\"\n",
    "\n",
    "            curr_time = cl_split[1]\n",
    "            dt_string = curr_year + \"-\" + curr_date + \" \" + curr_time\n",
    "            curr_dt = datetime.strptime(dt_string, '%y-%m-%d %H:%M:%S.%f')\n",
    "            cdt_unix = curr_dt.timestamp()\n",
    "\n",
    "            client_pp = cdt_unix - client_rec\n",
    "            session_latencies[curr_frame_loc][8] = client_pp\n",
    "                 \n",
    "#     # Delete non-positive results rows\n",
    "#     for nr_i in range(len(negative_results)):\n",
    "#         curr_nr = negative_results[nr_i]\n",
    "#         cnr_loc = np.where(session_latencies[:,0] == float(curr_nr))[0][0]\n",
    "#         session_latencies = np.delete(session_latencies, cnr_loc, 0)\n",
    "                \n",
    "    #print(clients)\n",
    "    #print(session_latencies)\n",
    "    return session_latencies, succesful_recognitions, succesful_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current log file is:\n",
      "[1571911280.0, '../logs/log_2019-10-24 13:01:20.txt']\n",
      "\n",
      "(117, 15)\n",
      "(array([10420112.]), array([117]))\n",
      "(0,)\n",
      "Client pre-processing: nan +- nan\n",
      "(0,)\n",
      "Data transfer: nan +- nan\n",
      "(115,)\n",
      "SIFT feature extraction: 1.99604034423828 +- 0.41391197854373907\n",
      "(6,)\n",
      "PCA dimension reduction: 1.72555446624756 +- 0.7343559893289353\n",
      "(113,)\n",
      "FV encoding with GMM: 1.58095359802246 +- 0.1761640405332451\n",
      "(6,)\n",
      "LSH NN searching: 1.652956008911135 +- 1.1822711646738044\n",
      "(114,)\n",
      "Template matching: 0.12 +- 0.032707066995617824\n",
      "(0,)\n",
      "Client post-processing: nan +- nan\n",
      "Overall latency: nan +- nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacky/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:63: RuntimeWarning: Mean of empty slice\n",
      "/home/jacky/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:63: RuntimeWarning: invalid value encountered in less\n"
     ]
    }
   ],
   "source": [
    "# rewriting to consider just one server instance\n",
    "\n",
    "# Sort by UNIX timestamp in ascending order\n",
    "log_files = sorted(log_files, key=operator.itemgetter(0))\n",
    "\n",
    "# for cf_i in range(len(log_files)):\n",
    "#     if cf_i % 2 == 0:\n",
    "#         # only consider pairs of files together\n",
    "\n",
    "latest_log = log_files[-1]\n",
    "\n",
    "print(\"Current log file is:\")\n",
    "print(latest_log)\n",
    "\n",
    "# calling location of file\n",
    "ll_file = latest_log[1]\n",
    "log_time = latest_log[0]\n",
    "\n",
    "# Collecting logcat file from Android Studio output\n",
    "logcat_file = \"/home/jacky/Desktop/mobile_ar_system/ar_apps/cloudridar_movieapp_client/logcat.txt\"\n",
    "\n",
    "# Send file names to parser\n",
    "log_results = single_log_parser(ll_file, logcat_file)\n",
    "\n",
    "tasks = {\n",
    "    1 : \"Client pre-processing: \",\n",
    "    2 : \"Data transfer: \",\n",
    "    3 : \"SIFT feature extraction: \",\n",
    "    4 : \"PCA dimension reduction: \",\n",
    "    5 : \"FV encoding with GMM: \",\n",
    "    6 : \"LSH NN searching: \",\n",
    "    7 : \"Template matching: \",\n",
    "    8 : \"Client post-processing: \"\n",
    "}\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "all_results = log_results[0]\n",
    "new_requests = log_results[1]\n",
    "cache_requests = log_results[2]\n",
    "\n",
    "# nr_results = all_results\n",
    "# cr_results = all_results\n",
    "\n",
    "# for i in range(len(cache_requests)):\n",
    "#     curr_cr = cache_requests[i]\n",
    "#     cnr_loc = np.where(nr_results[:,0] == float(curr_cr))[0][0]\n",
    "#     nr_results = np.delete(nr_results, cnr_loc, 0)\n",
    "\n",
    "all_results = all_results[~np.all(all_results == 0, axis=1)]\n",
    "all_results[all_results == 0] = np.nan\n",
    "print(np.shape(all_results))\n",
    "print(np.unique(all_results[:,14], return_counts=True))\n",
    "    \n",
    "overall_latency_med = 0\n",
    "overall_latency_std = 0\n",
    "for i in range(len(all_results[:,0])):\n",
    "    curr_index = i + 1 \n",
    "    if curr_index <= len(tasks):\n",
    "        curr_col = all_results[:,curr_index]\n",
    "        \n",
    "        # removing outliers\n",
    "        curr_col = curr_col[abs(curr_col - np.nanmean(curr_col)) < 2 * np.nanstd(curr_col)]\n",
    "        print(np.shape(curr_col))\n",
    "        \n",
    "        med_val = np.nanmedian(curr_col)\n",
    "        std_val = np.nanstd(curr_col)\n",
    "\n",
    "        overall_latency_med += med_val\n",
    "        overall_latency_std += std_val\n",
    "\n",
    "        print(tasks[curr_index] + str(med_val) + \" +- \" + str(std_val))\n",
    "\n",
    "print(\"Overall latency: \" + str(overall_latency_med) + \" +- \" + str(std_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(105, 15)\n",
      "(array([10420112.]), array([105]))\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-ea6a9023b6f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munique_ips\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mresults_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mresults_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0munique_ips\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "# multiple files and clients - combining together different logs\n",
    "\n",
    "# Sort by UNIX timestamp in ascending order\n",
    "log_files = sorted(log_files, key=operator.itemgetter(0))\n",
    "\n",
    "files_to_consider = log_files[-1]\n",
    "\n",
    "results_array = np.zeros([1,15])\n",
    "\n",
    "# for i in range(len(files_to_consider)):\n",
    "#     curr_log = files_to_consider[i]\n",
    "#     file_name = curr_log[1]\n",
    "    \n",
    "#     # Collecting logcat file from Android Studio output\n",
    "#     logcat_file = \"/home/jacky/Desktop/mobile_ar_system/ar_apps/cloudridar_movieapp_client/logcat.txt\"\n",
    "\n",
    "#     # Send file names to parser\n",
    "#     log_results = single_log_parser(file_name, logcat_file)\n",
    "#     lat_results = log_results[0]\n",
    "#     #print(lat_results)\n",
    "    \n",
    "#     results_array = np.append(results_array, lat_results, axis=0)\n",
    "\n",
    "file_name = files_to_consider[1]\n",
    "logcat_file = \"/home/jacky/Desktop/mobile_ar_system/ar_apps/cloudridar_movieapp_client/logcat.txt\"\n",
    "\n",
    "# Send file names to parser\n",
    "results_array = single_log_parser(file_name, logcat_file)[0]\n",
    "\n",
    "results_array = results_array[~np.all(results_array == 0, axis=1)]\n",
    "results_array[results_array == 0] = np.nan\n",
    "print(np.shape(results_array))\n",
    "\n",
    "unique_ips = np.unique(results_array[:,14],return_counts=True)\n",
    "print(unique_ips)\n",
    "\n",
    "results_array[results_array[:,-1] == unique_ips[0][1]] = np.nan\n",
    "print(results_array[0])\n",
    "                       \n",
    "tasks = {\n",
    "    1 : \"Client pre-processing: \",\n",
    "    2 : \"Data transfer: \",\n",
    "    3 : \"SIFT feature extraction: \",\n",
    "    4 : \"PCA dimension reduction: \",\n",
    "    5 : \"FV encoding with GMM: \",\n",
    "    6 : \"LSH NN searching: \",\n",
    "    7 : \"Template matching: \",\n",
    "    8 : \"Client post-processing: \"\n",
    "}\n",
    "\n",
    "print(\"\")\n",
    "    \n",
    "overall_latency_med = 0\n",
    "overall_latency_std = 0\n",
    "for i in range(len(results_array[:,0])):\n",
    "    curr_index = i + 1 \n",
    "    if curr_index <= len(tasks):\n",
    "        curr_col = results_array[:,curr_index]\n",
    "        \n",
    "        # removing outliers\n",
    "        curr_col = curr_col[abs(curr_col - np.nanmean(curr_col)) < 2 * np.nanstd(curr_col)]\n",
    "        \n",
    "        med_val = np.nanmedian(curr_col)\n",
    "        std_val = np.nanstd(curr_col)\n",
    "        \n",
    "        if med_val == np.nan:\n",
    "            med_val = 0\n",
    "\n",
    "        overall_latency_med += med_val\n",
    "        overall_latency_std += std_val\n",
    "\n",
    "        print(tasks[curr_index] + str(med_val) + \" +- \" + str(std_val))\n",
    "\n",
    "print(\"Overall latency: \" + str(overall_latency_med) + \" +- \" + str(std_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
