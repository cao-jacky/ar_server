{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import operator\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "\n",
    "plt.rc('text', usetex=True)\n",
    "rc('font',**{'family':'serif','serif':['Times']})\n",
    "plt.rcParams['text.latex.preamble'] = [r'\\boldmath']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Searching for the log and error files and storing them into lists\n",
    "log_folder = \"../logs\"\n",
    "folder_files = []\n",
    "\n",
    "for r, d, f in os.walk(log_folder):\n",
    "    for file in f:\n",
    "        if '.txt' in file:\n",
    "            folder_files.append(os.path.join(r, file))\n",
    "\n",
    "log_files = []\n",
    "error_files = []\n",
    "            \n",
    "for curr_file in folder_files:\n",
    "    cf_orig = curr_file\n",
    "    curr_file = curr_file.split(\"/\")[2].split(\"_\")\n",
    "    \n",
    "    cf_type = curr_file[0]\n",
    "    cf_time = curr_file[1].split(\".txt\")[0]\n",
    "    \n",
    "    cft_obj = datetime.strptime(cf_time, '%Y-%m-%d %H:%M:%S')\n",
    "    cft_unix = cft_obj.timestamp()\n",
    "    \n",
    "    file_info = [cft_unix, cf_orig]\n",
    "    \n",
    "    globals()[str(cf_type)+\"_files\"].append(file_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Looking through the cache log and recognition log files \n",
    "\n",
    "def log_parser(cl_name, rl_name, lc_file):    \n",
    "    # number of lines in file\n",
    "    with open(cl_name) as f:\n",
    "        csf_num_lines = sum(1 for _ in f)\n",
    "    with open(rl_name) as g:\n",
    "        rsf_num_lines = sum(1 for _ in g)\n",
    "    with open(lc_file) as g:\n",
    "        lcf_num_lines = sum(1 for _ in g)\n",
    "    \n",
    "    # no of frames dealt with\n",
    "    no_frames = open(cl_name, 'r').read().count(\"res sent\")\n",
    "    \n",
    "    # read file contents of logs\n",
    "    cs_file = open(cl_name, \"r\").readlines()\n",
    "    rs_file = open(rl_name, \"r\").readlines()\n",
    "    logcat_file = open(lc_file, \"r\").readlines()\n",
    "    \n",
    "    # storing current frame being considered\n",
    "    cf_ticker = []\n",
    "    \n",
    "    # array for latencies which are in units of ms\n",
    "    session_latencies = np.zeros([no_frames, 9])\n",
    "    transfer_latencies = np.zeros([no_frames, 5])\n",
    "    \n",
    "    negative_results = []\n",
    "    \n",
    "    no_frames_considered = 0\n",
    "    # looping over cache log\n",
    "    for cs_line in range(csf_num_lines):\n",
    "        if not cs_line < 8:\n",
    "            if len(cf_ticker) == 4:\n",
    "                cf_ticker = []\n",
    "                no_frames_considered += 1\n",
    "            curr_line = cs_file[cs_line]\n",
    "            if curr_line == \"\\n\":\n",
    "                pass\n",
    "            else:\n",
    "                cl_split = curr_line.split(\" \")\n",
    "\n",
    "                curr_frame = cl_split[1]\n",
    "                cf_ticker.append(curr_frame)\n",
    "                \n",
    "                if len(cf_ticker) == 1:\n",
    "                    session_latencies[no_frames_considered][0] = curr_frame\n",
    "                    transfer_latencies[no_frames_considered][0] = curr_frame\n",
    "                    \n",
    "                transfer_latencies[no_frames_considered][len(cf_ticker)] = cl_split[-1]\n",
    "                \n",
    "                if \"res sent, marker#: 0\" in curr_line:\n",
    "                    negative_results.append(curr_frame)\n",
    "                #print(cl_split)\n",
    "    \n",
    "    # storing the index of where the current frame is, in session_latencies\n",
    "    sl_where = 0\n",
    "        \n",
    "    # looping over recognition log\n",
    "    for rs_line in range(rsf_num_lines):\n",
    "        if not rs_line < 5:\n",
    "            curr_line = rs_file[rs_line]\n",
    "            \n",
    "            if not curr_line == \"\\n\":\n",
    "                cl_split = curr_line.split(\" \")\n",
    "            \n",
    "            if \"received, filesize:\" in curr_line:\n",
    "                curr_frame_no = cl_split[1]\n",
    "                sl_where = np.where(session_latencies[:,0] == float(curr_frame_no))[0][0]\n",
    "                sift_extract_time = 0\n",
    "                \n",
    "            if not curr_frame_no in negative_results:\n",
    "                if \"ExtractSift time total = \" in curr_line:\n",
    "                    indiv_sift_time = float(re.search(\"ExtractSift time total =(.*)ms\", curr_line).group(1))\n",
    "                    sift_extract_time += indiv_sift_time\n",
    "\n",
    "                if \"sift res\" in curr_line:\n",
    "                    sift_time = float(cl_split[-1]) * 1000\n",
    "                    session_latencies[sl_where][3] = sift_time\n",
    "\n",
    "                    #print(sift_extract_time, sift_time)\n",
    "\n",
    "                if \"pca encoding time\" in curr_line:\n",
    "                    pca_time = float(cl_split[-1]) * 1000\n",
    "                    session_latencies[sl_where][4] = pca_time\n",
    "\n",
    "                if \"fisher encoding time\" in curr_line:\n",
    "                    fsh_time = float(cl_split[-1]) * 1000\n",
    "                    session_latencies[sl_where][5] = fsh_time\n",
    "\n",
    "                if \"MatchSiftData time\" in curr_line:\n",
    "                    msd_time = float(cl_split[-2]) \n",
    "                    session_latencies[sl_where][6] = msd_time\n",
    "\n",
    "                if \"FindHomography time\" in curr_line:\n",
    "                    fhy_time = float(cl_split[-2])\n",
    "                    session_latencies[sl_where][7] = fhy_time\n",
    "\n",
    "    # Client times\n",
    "    pp_begin_uxtime = 0\n",
    "    frame_sent_ux_time = 0 \n",
    "    curr_frame_no = 0\n",
    "    curr_frame_loc = 0\n",
    "    \n",
    "    # offset of phone in seconds\n",
    "    android_offset = 3.330\n",
    "    \n",
    "    for lg_line in range(lcf_num_lines):\n",
    "        curr_line = logcat_file[lg_line]\n",
    "        cl_split = curr_line.split(\" \")\n",
    "        \n",
    "        if \"get gray scaled frame data at\" in curr_line:\n",
    "            pp_begin_uxtime = float(cl_split[-1])\n",
    "        \n",
    "        if \"sent with size\" in curr_line:\n",
    "            # find frame number\n",
    "            frame_no = float(re.search(\"frame(.*)sent with size\", curr_line).group(1))\n",
    "            \n",
    "            if not frame_no in negative_results:\n",
    "                # check for existence of frame number in server results\n",
    "                sl_where = np.where(session_latencies[:,0] == frame_no)[0]\n",
    "\n",
    "                if not sl_where.size == 0:\n",
    "                    curr_frame_no = frame_no\n",
    "                    curr_frame_loc = sl_where[0]\n",
    "\n",
    "                    frame_sent_ux_time = float(cl_split[-1]) \n",
    "\n",
    "                    pp_time = frame_sent_ux_time - pp_begin_uxtime\n",
    "                    session_latencies[curr_frame_loc][1] = pp_time\n",
    "\n",
    "                    server_rec = transfer_latencies[curr_frame_loc][1]\n",
    "                    data_transfer = abs(server_rec - (frame_sent_ux_time/1000) - android_offset)\n",
    "                    #print((frame_sent_ux_time/1000),android_offset, server_rec)\n",
    "                    session_latencies[curr_frame_loc][2] = data_transfer\n",
    "\n",
    "        if \"res received at\" in curr_line:\n",
    "            client_rec = (float(cl_split[-1]) / 1000)\n",
    "\n",
    "        if \"OMX-VDEC-1080P: Video slvp perflock acquired\" in curr_line:\n",
    "            curr_date = cl_split[0]\n",
    "            curr_year = \"19\"\n",
    "\n",
    "            curr_time = cl_split[1]\n",
    "            dt_string = curr_year + \"-\" + curr_date + \" \" + curr_time\n",
    "            curr_dt = datetime.strptime(dt_string, '%y-%m-%d %H:%M:%S.%f')\n",
    "            cdt_unix = curr_dt.timestamp()\n",
    "\n",
    "            client_pp = cdt_unix - client_rec\n",
    "            session_latencies[curr_frame_loc][8] = client_pp\n",
    "                 \n",
    "    # Delete non-positive results rows\n",
    "    for nr_i in range(len(negative_results)):\n",
    "        curr_nr = negative_results[nr_i]\n",
    "        cnr_loc = np.where(session_latencies[:,0] == float(curr_nr))[0][0]\n",
    "        session_latencies = np.delete(session_latencies, cnr_loc, 0)\n",
    "                \n",
    "    return session_latencies\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current two log files are:\n",
      "[1570093332.0, '1', '../logs/log_1_2019-10-03 12:02:12.txt'] [1570093335.0, '0', '../logs/log_0_2019-10-03 12:02:15.txt']\n",
      "\n",
      "\n",
      "Client pre-processing: 12.5 +- 4.199999999999999\n",
      "Data transfer: 6.65553505897522 +- 0.039528036751684835\n",
      "SIFT feature extraction: 3.53193283081055 +- 1.3141686848760223\n",
      "PCA dimension reduction: 3.59857082366943 +- 0.9434871648887053\n",
      "FV encoding with GMM: 3.40092182159424 +- 0.8647845739730776\n",
      "LSH NN searching: 1.275 +- 0.16930445948054648\n",
      "Template matching: 4.665 +- 0.6511405378257447\n",
      "Client post-processing: 1.0830000638961792 +- 0.01390103476069095\n",
      "Overall latency: 36.709960598945614 +- 0.01390103476069095\n"
     ]
    }
   ],
   "source": [
    "# Parsing log files \n",
    "\n",
    "server_mode = {\n",
    "    '1': \"recognition server\",\n",
    "    '0': \"cache server\"\n",
    "}\n",
    "\n",
    "# Sort by UNIX timestamp in ascending order\n",
    "log_files = sorted(log_files, key=operator.itemgetter(0))\n",
    "\n",
    "# for cf_i in range(len(log_files)):\n",
    "#     if cf_i % 2 == 0:\n",
    "#         # only consider pairs of files together\n",
    "\n",
    "recog_server = log_files[-2]\n",
    "cache_server = log_files[-1]\n",
    "\n",
    "print(\"Current two log files are:\")\n",
    "print(recog_server, cache_server)\n",
    "\n",
    "rs_file = recog_server[2]\n",
    "cs_file = cache_server[2]\n",
    "\n",
    "# perform two checks to make sure correct files are being used\n",
    "rs_mode = recog_server[1]\n",
    "cs_mode = cache_server[1]\n",
    "\n",
    "if not (rs_mode == \"1\") and not (cs_mode == \"0\"):\n",
    "    print(\"An error occurred in servers start up, remove either following files to correct:\")\n",
    "    print(rs_file)\n",
    "    print(cs_file)\n",
    "    pass\n",
    "\n",
    "rs_time = recog_server[0]\n",
    "cs_time = cache_server[0]\n",
    "\n",
    "if not (cs_time - rs_time) < 10:\n",
    "    print(\"Servers were started too late apart - start them within 10 s of each other\")\n",
    "    pass\n",
    "\n",
    "# Collecting logcat file from Android Studio output\n",
    "logcat_file = \"/home/jacky/Desktop/mobile_ar_system/ar_apps/cloudridar_movieapp_client/logcat.txt\"\n",
    "\n",
    "# Send file names to parser\n",
    "parsed_results = log_parser(cs_file, rs_file, logcat_file)\n",
    "\n",
    "tasks = {\n",
    "    1 : \"Client pre-processing: \",\n",
    "    2 : \"Data transfer: \",\n",
    "    3 : \"SIFT feature extraction: \",\n",
    "    4 : \"PCA dimension reduction: \",\n",
    "    5 : \"FV encoding with GMM: \",\n",
    "    6 : \"LSH NN searching: \",\n",
    "    7 : \"Template matching: \",\n",
    "    8 : \"Client post-processing: \"\n",
    "}\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "overall_latency_med = 0\n",
    "overall_latency_std = 0\n",
    "for i in range(len(parsed_results[:,0])):\n",
    "    curr_index = i + 1 \n",
    "    if curr_index <= len(tasks):\n",
    "        med_val = np.median(parsed_results[:,curr_index])\n",
    "        std_val = np.std(parsed_results[:,curr_index])\n",
    "\n",
    "        overall_latency_med += med_val\n",
    "        overall_latency_std += std_val\n",
    "\n",
    "        print(tasks[curr_index] + str(med_val) + \" +- \" + str(std_val))\n",
    "\n",
    "print(\"Overall latency: \" + str(overall_latency_med) + \" +- \" + str(std_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "code_folding": [
     126
    ]
   },
   "outputs": [],
   "source": [
    "def single_log_parser(log_file, lc_file):    \n",
    "    # number of lines in file\n",
    "    with open(log_file) as f:\n",
    "        log_num_lines = sum(1 for _ in f)\n",
    "    with open(lc_file) as g:\n",
    "        lcf_num_lines = sum(1 for _ in g)\n",
    "    \n",
    "    # no of frames dealt with\n",
    "    no_frames = open(log_file, 'r').read().count(\"received, filesize\")\n",
    "    \n",
    "    # read file contents of logs\n",
    "    lf_file = open(log_file, \"r\").readlines()\n",
    "    logcat_file = open(lc_file, \"r\").readlines()\n",
    "    \n",
    "    # storing current frame being considered\n",
    "    cf_ticker = []\n",
    "    \n",
    "    # requests\n",
    "    requests = []\n",
    "    \n",
    "    # arrays for latencies which are in units of ms\n",
    "    session_latencies = np.zeros([no_frames, 14])\n",
    "    transfer_latencies = np.zeros([no_frames, 5])\n",
    "    \n",
    "    negative_results = []\n",
    "    \n",
    "    cf_considered = 0\n",
    "    # looping over log\n",
    "    for lf_line in range(log_num_lines):\n",
    "        curr_line = lf_file[lf_line] \n",
    "        \n",
    "        if not curr_line == \"\\n\":\n",
    "            cl_split = curr_line.split(\" \")\n",
    "        \n",
    "        if \"received, filesize:\" in curr_line:\n",
    "            curr_frame_no = cl_split[1]\n",
    "            if session_latencies[cf_considered][0] == 0:\n",
    "                session_latencies[cf_considered][0] = float(curr_frame_no)\n",
    "            # averaging multiple times together \n",
    "            sift_points = []\n",
    "            match_sift = []\n",
    "\n",
    "        if \"SIFT points extracted in time\" in curr_line:\n",
    "            sift_time = float(cl_split[-1]) * 1000\n",
    "            sift_points.append(sift_time)\n",
    "            sp_average = np.average(sift_points)\n",
    "            session_latencies[cf_considered][3] = sp_average\n",
    "\n",
    "        if \"PCA encoding time\" in curr_line:\n",
    "            pca_time = float(cl_split[-1]) * 1000\n",
    "            session_latencies[cf_considered][4] = pca_time\n",
    "\n",
    "        if \"Fisher Vector encoding time\" in curr_line:\n",
    "            fsh_time = float(cl_split[-1]) * 1000\n",
    "            session_latencies[cf_considered][5] = fsh_time\n",
    "            \n",
    "        if \"time before matching\" in curr_line:\n",
    "            tbf_timestamp = float(cl_split[-1])\n",
    "\n",
    "        if \"LSH NN searching time\" in curr_line:\n",
    "            lshnn_time = float(cl_split[-1]) * 1000\n",
    "            session_latencies[cf_considered][6] = lshnn_time\n",
    "            \n",
    "        if \"after matching\" in curr_line:\n",
    "            af_time = float(cl_split[-1])\n",
    "            fhy_time = (af_time - tbf_timestamp) * 1000\n",
    "            #session_latencies[cf_considered][7] = fhy_time\n",
    "            \n",
    "        if \"MatchSiftData time\" in curr_line:\n",
    "            msd_time = float(cl_split[-2])\n",
    "            match_sift.append(msd_time)\n",
    "            msd_average = np.average(msd_time)\n",
    "            session_latencies[cf_considered][7] = msd_average\n",
    "            \n",
    "        if \"Matching features\" in curr_line:\n",
    "            mf_percentage = float(cl_split[-2].replace('%', ''))\n",
    "            session_latencies[cf_considered][9] = mf_percentage\n",
    "            \n",
    "        # was the cache used and was it succesful\n",
    "        if \"Cache query - time before matching:\" in curr_line:\n",
    "            session_latencies[cf_considered][11] = 1\n",
    "        \n",
    "        if \"Added item to cache\" in curr_line: \n",
    "            session_latencies[cf_considered][13] = 1\n",
    "        \n",
    "        if \"res sent, marker#:\" in curr_line:\n",
    "            marker_no = float(cl_split[5])\n",
    "            session_latencies[cf_considered][10] = marker_no\n",
    "            \n",
    "            cache_query = session_latencies[cf_considered][11]\n",
    "            \n",
    "            mf_percentage = session_latencies[cf_considered][9]\n",
    "            if (cache_query) and (0 < mf_percentage < 100 ) and (marker_no == 1):\n",
    "                session_latencies[cf_considered][12] = 1\n",
    "                \n",
    "            cf_considered += 1\n",
    "\n",
    "    succesful_recognitions = []\n",
    "    succesful_cache = []\n",
    "    # search through all the frames\n",
    "    for i in range(len(session_latencies)):\n",
    "        curr_frame = session_latencies[i]\n",
    "        \n",
    "        cf_id = curr_frame[0]\n",
    "        cf_recog = curr_frame[10]\n",
    "        cf_cache = curr_frame[12] # cache queried\n",
    "        \n",
    "        cf_add_cache = curr_frame[13] # item was added to cache\n",
    "        \n",
    "        if int(cf_recog) == 1:\n",
    "            # frame has had succesful recognition performed or cache recognition\n",
    "            \n",
    "            if int(cf_add_cache):\n",
    "                succesful_recognitions.append(cf_id)\n",
    "            else:\n",
    "                succesful_cache.append(cf_id)\n",
    "        \n",
    "    # Client times\n",
    "    pp_begin_uxtime = 0\n",
    "    frame_sent_ux_time = 0 \n",
    "    curr_frame_no = 0\n",
    "    curr_frame_loc = 0\n",
    "    \n",
    "    # offset of phone in seconds\n",
    "    android_offset = 3.330\n",
    "    \n",
    "    for lg_line in range(lcf_num_lines):\n",
    "        curr_line = logcat_file[lg_line]\n",
    "        cl_split = curr_line.split(\" \")\n",
    "        \n",
    "        if \"get gray scaled frame data at\" in curr_line:\n",
    "            pp_begin_uxtime = float(cl_split[-1])\n",
    "        \n",
    "        if \"sent with size\" in curr_line:\n",
    "            # find frame number\n",
    "            frame_no = float(re.search(\"frame(.*)sent with size\", curr_line).group(1))\n",
    "            \n",
    "            if not frame_no in negative_results:\n",
    "                # check for existence of frame number in server results\n",
    "                sl_where = np.where(session_latencies[:,0] == frame_no)[0]\n",
    "\n",
    "                if not sl_where.size == 0:\n",
    "                    curr_frame_no = frame_no\n",
    "                    curr_frame_loc = sl_where[0]\n",
    "\n",
    "                    frame_sent_ux_time = float(cl_split[-1]) \n",
    "\n",
    "                    pp_time = frame_sent_ux_time - pp_begin_uxtime\n",
    "                    session_latencies[curr_frame_loc][1] = pp_time\n",
    "\n",
    "                    server_rec = transfer_latencies[curr_frame_loc][1]\n",
    "                    data_transfer = abs(server_rec - (frame_sent_ux_time/1000) - android_offset)\n",
    "                    #print((frame_sent_ux_time/1000),android_offset, server_rec)\n",
    "                    session_latencies[curr_frame_loc][2] = data_transfer\n",
    "\n",
    "        if \"res received at\" in curr_line:\n",
    "            client_rec = (float(cl_split[-1]) / 1000)\n",
    "\n",
    "        if \"OMX-VDEC-1080P: Video slvp perflock acquired\" in curr_line:\n",
    "            curr_date = cl_split[0]\n",
    "            curr_year = \"19\"\n",
    "\n",
    "            curr_time = cl_split[1]\n",
    "            dt_string = curr_year + \"-\" + curr_date + \" \" + curr_time\n",
    "            curr_dt = datetime.strptime(dt_string, '%y-%m-%d %H:%M:%S.%f')\n",
    "            cdt_unix = curr_dt.timestamp()\n",
    "\n",
    "            client_pp = cdt_unix - client_rec\n",
    "            session_latencies[curr_frame_loc][8] = client_pp\n",
    "                 \n",
    "#     # Delete non-positive results rows\n",
    "#     for nr_i in range(len(negative_results)):\n",
    "#         curr_nr = negative_results[nr_i]\n",
    "#         cnr_loc = np.where(session_latencies[:,0] == float(curr_nr))[0][0]\n",
    "#         session_latencies = np.delete(session_latencies, cnr_loc, 0)\n",
    "                \n",
    "    #print(session_latencies)\n",
    "    return session_latencies, succesful_recognitions, succesful_cache\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current log file is:\n",
      "[1571649180.0, '../logs/log_2019-10-21 12:13:00.txt']\n",
      "\n",
      "Client pre-processing: 0.0 +- 1.9346977943739836\n",
      "Data transfer: 0.0 +- 433950879.43256825\n",
      "SIFT feature extraction: 2.165198326110839 +- 0.6162310892085621\n",
      "PCA dimension reduction: 1.773953437805175 +- 0.5844394343522284\n",
      "FV encoding with GMM: 8.79359245300293 +- 0.9149335615424836\n",
      "LSH NN searching: 2.682089805603025 +- 1.0178391393844077\n",
      "Template matching: 6.549954414367676 +- 5.976540669251882\n",
      "Client post-processing: 0.0 +- 0.40696760670973414\n",
      "Overall latency: 21.964788436889645 +- 0.40696760670973414\n"
     ]
    }
   ],
   "source": [
    "# rewriting to consider just one server instance\n",
    "\n",
    "# Sort by UNIX timestamp in ascending order\n",
    "log_files = sorted(log_files, key=operator.itemgetter(0))\n",
    "\n",
    "# for cf_i in range(len(log_files)):\n",
    "#     if cf_i % 2 == 0:\n",
    "#         # only consider pairs of files together\n",
    "\n",
    "latest_log = log_files[-1]\n",
    "\n",
    "print(\"Current log file is:\")\n",
    "print(latest_log)\n",
    "\n",
    "# calling location of file\n",
    "ll_file = latest_log[1]\n",
    "log_time = latest_log[0]\n",
    "\n",
    "# Collecting logcat file from Android Studio output\n",
    "logcat_file = \"/home/jacky/Desktop/mobile_ar_system/ar_apps/cloudridar_movieapp_client/logcat.txt\"\n",
    "\n",
    "# Send file names to parser\n",
    "log_results = single_log_parser(ll_file, logcat_file)\n",
    "\n",
    "tasks = {\n",
    "    1 : \"Client pre-processing: \",\n",
    "    2 : \"Data transfer: \",\n",
    "    3 : \"SIFT feature extraction: \",\n",
    "    4 : \"PCA dimension reduction: \",\n",
    "    5 : \"FV encoding with GMM: \",\n",
    "    6 : \"LSH NN searching: \",\n",
    "    7 : \"Template matching: \",\n",
    "    8 : \"Client post-processing: \"\n",
    "}\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "all_results = log_results[0]\n",
    "new_requests = log_results[1]\n",
    "cache_requests = log_results[2]\n",
    "\n",
    "nr_results = all_results\n",
    "cr_results = all_results\n",
    "\n",
    "for i in range(len(cache_requests)):\n",
    "    curr_cr = cache_requests[i]\n",
    "    cnr_loc = np.where(nr_results[:,0] == float(curr_cr))[0][0]\n",
    "    nr_results = np.delete(nr_results, cnr_loc, 0)\n",
    "    \n",
    "overall_latency_med = 0\n",
    "overall_latency_std = 0\n",
    "for i in range(len(nr_results[:,0])):\n",
    "    curr_index = i + 1 \n",
    "    if curr_index <= len(tasks):\n",
    "        med_val = np.median(nr_results[:,curr_index])\n",
    "        std_val = np.std(nr_results[:,curr_index])\n",
    "\n",
    "        overall_latency_med += med_val\n",
    "        overall_latency_std += std_val\n",
    "\n",
    "        print(tasks[curr_index] + str(med_val) + \" +- \" + str(std_val))\n",
    "\n",
    "print(\"Overall latency: \" + str(overall_latency_med) + \" +- \" + str(std_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
