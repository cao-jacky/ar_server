{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T14:02:56.399821Z",
     "start_time": "2019-12-09T14:02:56.152419Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import operator\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import mmap\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "\n",
    "plt.rc('text', usetex=True)\n",
    "rc('font',**{'family':'serif','serif':['Times']})\n",
    "plt.rcParams['text.latex.preamble'] = [r'\\boldmath']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T14:03:31.397624Z",
     "start_time": "2019-12-09T14:03:31.392951Z"
    }
   },
   "outputs": [],
   "source": [
    "# Searching through server logs\n",
    "server_log_folder = \"../logs_server/logs\"\n",
    "server_folder_files = []\n",
    "\n",
    "for r, d, f in os.walk(server_log_folder):\n",
    "    for file in f:\n",
    "        if '.txt' in file:\n",
    "            server_folder_files.append(os.path.join(r, file))\n",
    "\n",
    "server_log_files = []\n",
    "            \n",
    "for curr_file in server_folder_files:\n",
    "    cf_orig = curr_file\n",
    "    curr_file = curr_file.split(\"/\")[3].split(\"_\")\n",
    "    \n",
    "    cf_type = curr_file[0]\n",
    "    cf_time = curr_file[1].split(\".txt\")[0]\n",
    "    \n",
    "    cft_obj = datetime.strptime(cf_time, '%Y-%m-%d %H:%M:%S')\n",
    "    cft_unix = cft_obj.timestamp()\n",
    "    \n",
    "    file_info = [cft_unix, cf_orig]\n",
    "\n",
    "    globals()[\"server_log_files\"].append(file_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T14:03:49.911318Z",
     "start_time": "2019-12-09T14:03:49.900102Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# useful functions\n",
    "\n",
    "def find_nearest(array, value):\n",
    "    # Returning nearest value and index\n",
    "    array = np.asarray(array)\n",
    "    idx = (np.abs(array - value)).argmin()\n",
    "    return array[idx], idx\n",
    "\n",
    "def frames_finder_device(file_name):\n",
    "    file_frames = []\n",
    "    with open(file_name) as search:\n",
    "        for line in search:\n",
    "            line = line.rstrip()  # remove '\\n' at end of line\n",
    "            if \"sent with size\" in line:\n",
    "                frame_no = int(re.findall(r'frame(.+?)sent with size', line)[0])\n",
    "                file_frames.append(frame_no)         \n",
    "    return file_frames\n",
    "    \n",
    "def frames_finder_server(file_name):\n",
    "    file_frames = []\n",
    "    with open(file_name) as search:\n",
    "        for line in search:\n",
    "            line = line.rstrip()  # remove '\\n' at end of line\n",
    "            if \"received, filesize:\" in line:\n",
    "                frame_no = int(re.findall(r'Frame(.+?)received, filesize:', line)[0])\n",
    "                file_frames.append(frame_no)\n",
    "    return file_frames\n",
    "\n",
    "def frames_comparer(server_log, device_log):\n",
    "    device_frames = frames_finder_device(device_log)\n",
    "    server_frames = frames_finder_server(server_log)\n",
    "    \n",
    "    list_comparison = set(device_frames) & set(server_frames)\n",
    "    device_server_percent = (len(list_comparison) / len(server_frames)) * 100\n",
    "    \n",
    "    if device_server_percent > 80:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T14:06:01.889612Z",
     "start_time": "2019-12-09T14:06:01.876892Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def log_parser(server_lf):    \n",
    "    # number of lines in file\n",
    "    with open(server_lf) as f:\n",
    "        num_lines_s = sum(1 for _ in f)\n",
    "    \n",
    "    # number of frames sent from the device\n",
    "    no_frames = open(server_lf, 'r').read().count(\"received, filesize:\")\n",
    "    \n",
    "    # reading the file contents of both logs\n",
    "    contents_s = open(server_lf, \"r\").readlines()\n",
    "    \n",
    "    # storing current frame being considered\n",
    "    cf_ticker = []\n",
    "    \n",
    "    # requests\n",
    "    requests = []\n",
    "    \n",
    "    # arrays for latencies which are in units of ms\n",
    "    session_latencies = np.zeros([no_frames, 18])\n",
    "    transfer_latencies = np.zeros([no_frames, 5])\n",
    "    \n",
    "    clients = []\n",
    "    \n",
    "    cf_considered = 0\n",
    "    # looping over server log\n",
    "    for lf_line in range(num_lines_s):\n",
    "        curr_line = contents_s[lf_line] \n",
    "        \n",
    "        if not curr_line == \"\\n\":\n",
    "            cl_split = curr_line.split(\" \")\n",
    "        \n",
    "        if \"received, filesize:\" in curr_line:\n",
    "            curr_frame_no = cl_split[1]\n",
    "            file_size = cl_split[4]\n",
    "            device_ip = cl_split[-1]\n",
    "            time_received = cl_split[6]\n",
    "            dip_int = device_ip.replace('.', '').replace('\\n', '')\n",
    "            if dip_int not in clients:\n",
    "                clients.append(dip_int)\n",
    "            if session_latencies[cf_considered][0] == 0:\n",
    "                session_latencies[cf_considered][0] = float(curr_frame_no)\n",
    "                session_latencies[cf_considered][14] = float(dip_int)\n",
    "                session_latencies[cf_considered][17] = float(file_size)\n",
    "                \n",
    "            s_receive = float(re.findall(\" at (.*)\\n\", curr_line)[0])\n",
    "            \n",
    "            # averaging multiple times together \n",
    "            sift_points = []\n",
    "            match_sift = []\n",
    "\n",
    "        if \"SIFT points extracted in time\" in curr_line:\n",
    "            sift_time = float(cl_split[-1]) * 1000\n",
    "            sift_points.append(sift_time)\n",
    "            sp_average = np.average(sift_points)\n",
    "            session_latencies[cf_considered][3] = sp_average\n",
    "\n",
    "        if \"PCA encoding time\" in curr_line:\n",
    "            pca_time = float(cl_split[-1]) * 1000\n",
    "            session_latencies[cf_considered][4] = pca_time\n",
    "\n",
    "        if \"Fisher Vector encoding time\" in curr_line:\n",
    "            fsh_time = float(cl_split[-1]) * 1000\n",
    "            session_latencies[cf_considered][5] = fsh_time\n",
    "            \n",
    "        if \"time before matching\" in curr_line:\n",
    "            tbf_timestamp = float(cl_split[-1])\n",
    "\n",
    "        if \"LSH NN searching time\" in curr_line:\n",
    "            lshnn_time = float(cl_split[-1]) * 1000\n",
    "            session_latencies[cf_considered][6] = lshnn_time\n",
    "            \n",
    "        if \"after matching\" in curr_line:\n",
    "            af_time = float(cl_split[-1])\n",
    "            fhy_time = (af_time - tbf_timestamp) * 1000\n",
    "            #session_latencies[cf_considered][7] = fhy_time\n",
    "            \n",
    "        if \"MatchSiftData time\" in curr_line:\n",
    "            msd_time = float(cl_split[-2])\n",
    "            match_sift.append(msd_time)\n",
    "            msd_average = np.average(msd_time)\n",
    "            session_latencies[cf_considered][7] = msd_average\n",
    "            \n",
    "        if \"Matching features\" in curr_line:\n",
    "            mf_percentage = float(cl_split[-2].replace('%', ''))\n",
    "            session_latencies[cf_considered][9] = mf_percentage\n",
    "            \n",
    "        # was the cache used and was it succesful\n",
    "        if \"Cache query - time before matching:\" in curr_line:\n",
    "            session_latencies[cf_considered][11] = 1\n",
    "        \n",
    "        if \"Added item to cache\" in curr_line: \n",
    "            session_latencies[cf_considered][13] = 1\n",
    "        \n",
    "        if \"res sent, marker#:\" in curr_line:\n",
    "            marker_no = float(cl_split[5])\n",
    "            session_latencies[cf_considered][10] = marker_no\n",
    "            \n",
    "            cache_query = session_latencies[cf_considered][11]\n",
    "            \n",
    "            mf_percentage = session_latencies[cf_considered][9]\n",
    "            if (cache_query) and (0 < mf_percentage < 100 ) and (marker_no == 1):\n",
    "                session_latencies[cf_considered][12] = 1\n",
    "                \n",
    "            s_send = float(cl_split[-1])\n",
    "            s_total = (s_send - s_receive) * 1000\n",
    "            session_latencies[cf_considered][15] = s_total\n",
    "            \n",
    "            cf_considered += 1\n",
    "    return session_latencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T14:06:02.117314Z",
     "start_time": "2019-12-09T14:06:02.110320Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../logs_server/logs/log_2019-12-09 14:42:41.txt\n"
     ]
    }
   ],
   "source": [
    "print(server_log_files[-1][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T14:06:14.886806Z",
     "start_time": "2019-12-09T14:06:14.850846Z"
    }
   },
   "outputs": [],
   "source": [
    "parsed = log_parser(server_log_files[-1][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T14:55:14.061373Z",
     "start_time": "2019-12-09T14:55:14.051734Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Client pre-processing: 0 +- 0\n",
      "Data transfer: 0 +- 0\n",
      "SIFT feature extraction: 1.912434895833335 +- 0.633542398809233\n",
      "PCA dimension reduction: 1.8504858016967751 +- 0.6144360238090882\n",
      "FV encoding with GMM: 7.081985473632814 +- 0.616317657840921\n",
      "LSH NN searching: 2.61902809143066 +- 1.057119774502953\n",
      "Template matching: 0.11 +- 0.058823919525774815\n",
      "Client post-processing: 0 +- 0\n",
      "Overall latency: 13.573934262593585 +- 2.9802397744879703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacky/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:30: RuntimeWarning: Mean of empty slice\n",
      "/home/jacky/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:30: RuntimeWarning: invalid value encountered in less\n"
     ]
    }
   ],
   "source": [
    "# parsing matched server and client log files to extract latencies\n",
    "\n",
    "results_array = parsed\n",
    "\n",
    "results_array = results_array[~np.all(results_array == 0, axis=1)]\n",
    "results_array[results_array == 0] = np.nan\n",
    "\n",
    "tasks = {\n",
    "    1 : \"Client pre-processing: \",\n",
    "    2 : \"Data transfer: \",\n",
    "    3 : \"SIFT feature extraction: \",\n",
    "    4 : \"PCA dimension reduction: \",\n",
    "    5 : \"FV encoding with GMM: \",\n",
    "    6 : \"LSH NN searching: \",\n",
    "    7 : \"Template matching: \",\n",
    "    8 : \"Client post-processing: \"\n",
    "}\n",
    "\n",
    "print(\"\")\n",
    "    \n",
    "overall_latency_med = 0\n",
    "overall_latency_std = 0\n",
    "\n",
    "for i in range(len(results_array[:,0])):\n",
    "    curr_index = i + 1 \n",
    "    if curr_index <= len(tasks):\n",
    "        curr_col = results_array[:,curr_index]\n",
    "        \n",
    "        # removing outliers\n",
    "        curr_col = curr_col[abs(curr_col - np.nanmean(curr_col)) < 2 * np.nanstd(curr_col)]\n",
    "        \n",
    "        med_val = np.nanmedian(curr_col)\n",
    "        std_val = np.nanstd(curr_col)\n",
    "        \n",
    "        if np.isnan(med_val):\n",
    "            med_val = 0\n",
    "            std_val = 0\n",
    "\n",
    "        overall_latency_med += med_val\n",
    "        overall_latency_std += std_val\n",
    "\n",
    "        print(tasks[curr_index] + str(med_val) + \" +- \" + str(std_val))\n",
    "\n",
    "print(\"Overall latency: \" + str(overall_latency_med) + \" +- \" + str(overall_latency_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
