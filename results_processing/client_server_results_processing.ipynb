{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T12:27:10.226887Z",
     "start_time": "2019-11-27T12:27:09.980470Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import operator\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import mmap\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "\n",
    "plt.rc('text', usetex=True)\n",
    "rc('font',**{'family':'serif','serif':['Times']})\n",
    "plt.rcParams['text.latex.preamble'] = [r'\\boldmath']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T12:30:38.665023Z",
     "start_time": "2019-11-27T12:30:38.646245Z"
    }
   },
   "outputs": [],
   "source": [
    "# Searching through server logs\n",
    "server_log_folder = \"../logs_server/logs\"\n",
    "server_folder_files = []\n",
    "\n",
    "for r, d, f in os.walk(server_log_folder):\n",
    "    for file in f:\n",
    "        if '.txt' in file:\n",
    "            server_folder_files.append(os.path.join(r, file))\n",
    "\n",
    "server_log_files = []\n",
    "            \n",
    "for curr_file in server_folder_files:\n",
    "    cf_orig = curr_file\n",
    "    curr_file = curr_file.split(\"/\")[3].split(\"_\")\n",
    "    \n",
    "    cf_type = curr_file[0]\n",
    "    cf_time = curr_file[1].split(\".txt\")[0]\n",
    "    \n",
    "    cft_obj = datetime.strptime(cf_time, '%Y-%m-%d %H:%M:%S')\n",
    "    cft_unix = cft_obj.timestamp()\n",
    "    \n",
    "    file_info = [cft_unix, cf_orig]\n",
    "\n",
    "    globals()[\"server_log_files\"].append(file_info)\n",
    "    \n",
    "############################\n",
    "    \n",
    "# Searching through device logs \n",
    "device_logs_folder = \"../logs_devices/huawei\"\n",
    "devices_folder_files = []\n",
    "\n",
    "for r, d, f in os.walk(device_logs_folder):\n",
    "    for file in f:\n",
    "        if '.txt' in file:\n",
    "            devices_folder_files.append(os.path.join(r, file))\n",
    "            \n",
    "devices_log_files = []\n",
    "            \n",
    "for curr_file in devices_folder_files:\n",
    "    cf_orig = curr_file\n",
    "    curr_file = curr_file.split(\"/\")[-1].split(\"_\")\n",
    "    cf_time = float(curr_file[1].split(\".txt\")[0]) / 1000\n",
    "    \n",
    "    file_info = [str(cf_time), cf_orig]\n",
    "    globals()[\"devices_log_files\"].append(file_info)\n",
    "    \n",
    "# sort by unix_timestamp\n",
    "server_log_files = sorted(server_log_files, key=operator.itemgetter(0))\n",
    "devices_log_files = sorted(devices_log_files, key=operator.itemgetter(0))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T12:28:25.846902Z",
     "start_time": "2019-11-27T12:28:25.838338Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# useful functions\n",
    "\n",
    "def find_nearest(array, value):\n",
    "    # Returning nearest value and index\n",
    "    array = np.asarray(array)\n",
    "    idx = (np.abs(array - value)).argmin()\n",
    "    return array[idx], idx\n",
    "\n",
    "def frames_finder_device(file_name):\n",
    "    file_frames = []\n",
    "    with open(file_name) as search:\n",
    "        for line in search:\n",
    "            line = line.rstrip()  # remove '\\n' at end of line\n",
    "            if \"sent with size\" in line:\n",
    "                frame_no = int(re.findall(r'frame(.+?)sent with size', line)[0])\n",
    "                file_frames.append(frame_no)         \n",
    "    return file_frames\n",
    "    \n",
    "def frames_finder_server(file_name):\n",
    "    file_frames = []\n",
    "    with open(file_name) as search:\n",
    "        for line in search:\n",
    "            line = line.rstrip()  # remove '\\n' at end of line\n",
    "            if \"received, filesize:\" in line:\n",
    "                frame_no = int(re.findall(r'Frame(.+?)received, filesize:', line)[0])\n",
    "                file_frames.append(frame_no)\n",
    "    return file_frames\n",
    "\n",
    "def frames_comparer(server_log, device_log):\n",
    "    device_frames = frames_finder_device(device_log)\n",
    "    server_frames = frames_finder_server(server_log)\n",
    "    \n",
    "    list_comparison = set(device_frames) & set(server_frames)\n",
    "    device_server_percent = (len(list_comparison) / len(server_frames)) * 100\n",
    "    \n",
    "    if device_server_percent > 80:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T12:32:43.521006Z",
     "start_time": "2019-11-27T12:32:43.510325Z"
    }
   },
   "outputs": [],
   "source": [
    "# matching both server and device log files to as close together as possible \n",
    "\n",
    "# extracting server log file creation time\n",
    "sl_times = [lst[:1] for lst in server_log_files]\n",
    "sl_times = np.array(sl_times)\n",
    "\n",
    "matched_logs = []\n",
    "\n",
    "for i_dl in range(len(devices_log_files)):\n",
    "    curr_dl = devices_log_files[i_dl] # curr device log\n",
    "    dl_creation = int(float(curr_dl[0])) # int time for device log creation\n",
    "    \n",
    "    # finding closest server log file\n",
    "    closest_sl = find_nearest(sl_times, dl_creation)\n",
    "    csl_time = closest_sl[0]\n",
    "    csl_index = closest_sl[1]\n",
    "    server_log_file = server_log_files[csl_index] # select the logfile\n",
    "    \n",
    "    #print([server_log_file[1], curr_dl[1]])\n",
    "    compare_logs = frames_comparer(server_log_file[1], curr_dl[1])\n",
    "    if compare_logs:\n",
    "        matched_logs.append([server_log_file[1], curr_dl[1]]) # save only logfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T12:46:24.606111Z",
     "start_time": "2019-11-27T12:46:24.583427Z"
    },
    "code_folding": [
     73
    ]
   },
   "outputs": [],
   "source": [
    "def log_parser(server_lf, device_lf):    \n",
    "    # number of lines in file\n",
    "    with open(server_lf) as f:\n",
    "        num_lines_s = sum(1 for _ in f)\n",
    "    with open(device_lf) as g:\n",
    "        num_lines_d = sum(1 for _ in g)\n",
    "    \n",
    "    # number of frames sent from the device\n",
    "    no_frames = open(server_lf, 'r').read().count(\"received, filesize:\")\n",
    "    \n",
    "    # reading the file contents of both logs\n",
    "    contents_s = open(server_lf, \"r\").readlines()\n",
    "    contents_d = open(device_lf, \"r\").readlines()\n",
    "    \n",
    "    # storing current frame being considered\n",
    "    cf_ticker = []\n",
    "    \n",
    "    # requests\n",
    "    requests = []\n",
    "    \n",
    "    # arrays for latencies which are in units of ms\n",
    "    session_latencies = np.zeros([no_frames, 18])\n",
    "    transfer_latencies = np.zeros([no_frames, 5])\n",
    "    \n",
    "    clients = []\n",
    "    \n",
    "    cf_considered = 0\n",
    "    # looping over server log\n",
    "    for lf_line in range(num_lines_s):\n",
    "        curr_line = contents_s[lf_line] \n",
    "        \n",
    "        if not curr_line == \"\\n\":\n",
    "            cl_split = curr_line.split(\" \")\n",
    "        \n",
    "        if \"received, filesize:\" in curr_line:\n",
    "            curr_frame_no = cl_split[1]\n",
    "            file_size = cl_split[4]\n",
    "            device_ip = cl_split[-1]\n",
    "            time_received = cl_split[6]\n",
    "            dip_int = device_ip.replace('.', '').replace('\\n', '')\n",
    "            if dip_int not in clients:\n",
    "                clients.append(dip_int)\n",
    "            if session_latencies[cf_considered][0] == 0:\n",
    "                session_latencies[cf_considered][0] = float(curr_frame_no)\n",
    "                session_latencies[cf_considered][14] = float(dip_int)\n",
    "                session_latencies[cf_considered][17] = float(file_size)\n",
    "                \n",
    "            s_receive = float(re.findall(\" at (.*)\\n\", curr_line)[0])\n",
    "            \n",
    "            # averaging multiple times together \n",
    "            sift_points = []\n",
    "            match_sift = []\n",
    "\n",
    "        if \"SIFT points extracted in time\" in curr_line:\n",
    "            sift_time = float(cl_split[-1]) * 1000\n",
    "            sift_points.append(sift_time)\n",
    "            sp_average = np.average(sift_points)\n",
    "            session_latencies[cf_considered][3] = sp_average\n",
    "\n",
    "        if \"PCA encoding time\" in curr_line:\n",
    "            pca_time = float(cl_split[-1]) * 1000\n",
    "            session_latencies[cf_considered][4] = pca_time\n",
    "\n",
    "        if \"Fisher Vector encoding time\" in curr_line:\n",
    "            fsh_time = float(cl_split[-1]) * 1000\n",
    "            session_latencies[cf_considered][5] = fsh_time\n",
    "            \n",
    "        if \"time before matching\" in curr_line:\n",
    "            tbf_timestamp = float(cl_split[-1])\n",
    "\n",
    "        if \"LSH NN searching time\" in curr_line:\n",
    "            lshnn_time = float(cl_split[-1]) * 1000\n",
    "            session_latencies[cf_considered][6] = lshnn_time\n",
    "            \n",
    "        if \"after matching\" in curr_line:\n",
    "            af_time = float(cl_split[-1])\n",
    "            fhy_time = (af_time - tbf_timestamp) * 1000\n",
    "            #session_latencies[cf_considered][7] = fhy_time\n",
    "            \n",
    "        if \"MatchSiftData time\" in curr_line:\n",
    "            msd_time = float(cl_split[-2])\n",
    "            match_sift.append(msd_time)\n",
    "            msd_average = np.average(msd_time)\n",
    "            session_latencies[cf_considered][7] = msd_average\n",
    "            \n",
    "        if \"Matching features\" in curr_line:\n",
    "            mf_percentage = float(cl_split[-2].replace('%', ''))\n",
    "            session_latencies[cf_considered][9] = mf_percentage\n",
    "            \n",
    "        # was the cache used and was it succesful\n",
    "        if \"Cache query - time before matching:\" in curr_line:\n",
    "            session_latencies[cf_considered][11] = 1\n",
    "        \n",
    "        if \"Added item to cache\" in curr_line: \n",
    "            session_latencies[cf_considered][13] = 1\n",
    "        \n",
    "        if \"res sent, marker#:\" in curr_line:\n",
    "            marker_no = float(cl_split[5])\n",
    "            session_latencies[cf_considered][10] = marker_no\n",
    "            \n",
    "            cache_query = session_latencies[cf_considered][11]\n",
    "            \n",
    "            mf_percentage = session_latencies[cf_considered][9]\n",
    "            if (cache_query) and (0 < mf_percentage < 100 ) and (marker_no == 1):\n",
    "                session_latencies[cf_considered][12] = 1\n",
    "                \n",
    "            s_send = float(cl_split[-1])\n",
    "            s_total = (s_send - s_receive) * 1000\n",
    "            session_latencies[cf_considered][15] = s_total\n",
    "            \n",
    "            cf_considered += 1\n",
    "\n",
    "#     succesful_recognitions = []\n",
    "#     succesful_cache = []\n",
    "#     # search through all the frames\n",
    "#     for i in range(len(session_latencies)):\n",
    "#         curr_frame = session_latencies[i]\n",
    "        \n",
    "#         cf_id = curr_frame[0]\n",
    "#         cf_recog = curr_frame[10]\n",
    "#         cf_cache = curr_frame[12] # cache queried\n",
    "        \n",
    "#         cf_add_cache = curr_frame[13] # item was added to cache\n",
    "        \n",
    "#         if int(cf_recog) == 1:\n",
    "#             # frame has had succesful recognition performed or cache recognition\n",
    "            \n",
    "#             if int(cf_add_cache):\n",
    "#                 succesful_recognitions.append(cf_id)\n",
    "#             else:\n",
    "#                 succesful_cache.append(cf_id)\n",
    "        \n",
    "    # Client times\n",
    "    pp_begin_uxtime = 0\n",
    "    frame_sent_ux_time = 0 \n",
    "    curr_frame_no = 0\n",
    "    curr_frame_loc = 0\n",
    "    \n",
    "    # offset of phone in seconds\n",
    "    android_offset = 0\n",
    "    \n",
    "    for lf_line in range(num_lines_d):\n",
    "        curr_line = contents_d[lf_line]\n",
    "        cl_split = curr_line.split(\" \")\n",
    "\n",
    "        if \"get gray scaled frame data at\" in curr_line:\n",
    "            c_pre_begin = float(cl_split[-1])\n",
    "        \n",
    "        if \"sent with size\" in curr_line:\n",
    "            # find frame number\n",
    "            frame_no = float(re.search(\"frame(.*)sent with size\", curr_line).group(1))\n",
    "\n",
    "            sl_where = np.where(session_latencies[:,0] == frame_no)[0]\n",
    "\n",
    "            if not sl_where.size == 0:\n",
    "                curr_frame_no = frame_no\n",
    "                curr_frame_loc = sl_where[0]\n",
    "\n",
    "                c_send = float(cl_split[-1]) \n",
    "\n",
    "                c_pre = c_send - c_pre_begin\n",
    "                session_latencies[curr_frame_loc][1] = c_pre\n",
    "\n",
    "        if \"res received at\" in curr_line:\n",
    "            c_receive = (float(cl_split[-1]) / 1000)\n",
    "\n",
    "        if \"image border created:\" in curr_line:\n",
    "            curr_date = cl_split[0]\n",
    "            curr_year = \"19\"\n",
    "\n",
    "            curr_time = cl_split[1]\n",
    "            dt_string = curr_year + \"-\" + curr_date + \" \" + curr_time\n",
    "            curr_dt = datetime.strptime(dt_string, '%y-%m-%d %H:%M:%S.%f')\n",
    "            cdt_unix = curr_dt.timestamp()\n",
    "            \n",
    "            # time from client sending to client receiving\n",
    "            c_send_receive = (c_receive - (c_send/1000)) * 1000\n",
    "            \n",
    "            # total data transfer times \n",
    "            data_transfer = c_send_receive - session_latencies[curr_frame_loc][15]\n",
    "            dt_fraction = session_latencies[curr_frame_loc][17] / (session_latencies[curr_frame_loc][17] + 512)\n",
    "            data_transfer = dt_fraction * data_transfer\n",
    "            \n",
    "            session_latencies[curr_frame_loc][2] = data_transfer\n",
    "\n",
    "            # post-processing time\n",
    "            c_post = cdt_unix - c_receive\n",
    "            session_latencies[curr_frame_loc][8] = c_post * 1000\n",
    "            \n",
    "            c_total = c_pre + c_post\n",
    "            session_latencies[curr_frame_loc][16] = c_total\n",
    "                 \n",
    "    #print(session_latencies)\n",
    "    return session_latencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T12:56:57.178960Z",
     "start_time": "2019-11-27T12:56:57.087233Z"
    },
    "code_folding": [
     56,
     58
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Client pre-processing: 11.0 +- 2.9851554597864123\n",
      "Data transfer: 264.8790381694201 +- 15.790993282101912\n",
      "SIFT feature extraction: 1.74093246459961 +- 0.4914205959629532\n",
      "PCA dimension reduction: 0.44405460357666005 +- 0.521548442540035\n",
      "FV encoding with GMM: 7.39002227783203 +- 0.5605457351183846\n",
      "LSH NN searching: 0.58293342590332 +- 0.09909787150313927\n",
      "Template matching: 0.1 +- 0.036756583347221455\n",
      "Client post-processing: 6.999969482421875 +- 1.2472349320385085\n",
      "Overall latency: 293.1369504237536 +- 1.2472349320385085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacky/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:55: RuntimeWarning: invalid value encountered in less\n"
     ]
    }
   ],
   "source": [
    "# parsing matched server and client log files to extract latencies\n",
    "\n",
    "results_array = np.zeros([1,18])\n",
    "\n",
    "# counter = 1\n",
    "# for i in range(len(matched_logs)):\n",
    "#     curr_logs = matched_logs[i]\n",
    "\n",
    "#     curr_server_log = curr_logs[0]\n",
    "#     curr_device_log = curr_logs[1]\n",
    "    \n",
    "#     results = log_parser(curr_server_log, curr_device_log)\n",
    "    \n",
    "# #     if counter == len(matched_logs):\n",
    "# #         print(curr_server_log, curr_device_log)\n",
    "# #         results_array = np.append(results_array, results, axis=0)\n",
    "        \n",
    "#     results_array = np.append(results_array, results, axis=0)\n",
    "    \n",
    "#     counter += 1\n",
    "\n",
    "curr_server_log = server_log_files[-1][1]\n",
    "\n",
    "for i in range(len(devices_log_files)):\n",
    "    curr_log = devices_log_files[i]\n",
    "    curr_device_log = curr_log[1]\n",
    "    \n",
    "    results = log_parser(curr_server_log, curr_device_log)\n",
    "    results_array = np.append(results_array, results, axis=0)\n",
    "\n",
    "results_array = results_array[~np.all(results_array == 0, axis=1)]\n",
    "results_array[results_array == 0] = np.nan\n",
    "\n",
    "tasks = {\n",
    "    1 : \"Client pre-processing: \",\n",
    "    2 : \"Data transfer: \",\n",
    "    3 : \"SIFT feature extraction: \",\n",
    "    4 : \"PCA dimension reduction: \",\n",
    "    5 : \"FV encoding with GMM: \",\n",
    "    6 : \"LSH NN searching: \",\n",
    "    7 : \"Template matching: \",\n",
    "    8 : \"Client post-processing: \"\n",
    "}\n",
    "\n",
    "print(\"\")\n",
    "    \n",
    "overall_latency_med = 0\n",
    "overall_latency_std = 0\n",
    "for i in range(len(results_array[:,0])):\n",
    "    curr_index = i + 1 \n",
    "    if curr_index <= len(tasks):\n",
    "        curr_col = results_array[:,curr_index]\n",
    "        \n",
    "        # removing outliers\n",
    "        curr_col = curr_col[abs(curr_col - np.nanmean(curr_col)) < 2 * np.nanstd(curr_col)]\n",
    "        \n",
    "        med_val = np.nanmedian(curr_col)\n",
    "        std_val = np.nanstd(curr_col)\n",
    "\n",
    "        overall_latency_med += med_val\n",
    "        overall_latency_std += std_val`\n",
    "\n",
    "        print(tasks[curr_index] + str(med_val) + \" +- \" + str(std_val))\n",
    "\n",
    "print(\"Overall latency: \" + str(overall_latency_med) + \" +- \" + str(std_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
